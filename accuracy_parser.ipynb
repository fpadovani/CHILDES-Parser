{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83fecf55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/frapadovani/Desktop/stanza/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "import spacy\n",
    "from conllu import parse_incr\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ea69783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_conllu_gold(path):\n",
    "    \"\"\"\n",
    "    Load gold annotations from a CONLLU file.\n",
    "    Returns list of sentences, each as a list of dicts:\n",
    "    {\n",
    "        'form': str, \n",
    "        'upos': str, \n",
    "        'xpos': str, \n",
    "        'head': int, \n",
    "        'deprel': str\n",
    "    }\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for tokenlist in parse_incr(f):\n",
    "            tokens = []\n",
    "            for token in tokenlist:\n",
    "                if isinstance(token[\"id\"], tuple):  # skip multi-word tokens like \"2-3\"\n",
    "                    continue\n",
    "                tokens.append({\n",
    "                    \"form\": token[\"form\"],\n",
    "                    \"upos\": token[\"upostag\"],\n",
    "                    \"xpos\": token[\"xpostag\"],\n",
    "                    \"head\": token[\"head\"],\n",
    "                    \"deprel\": token[\"deprel\"]\n",
    "                })\n",
    "            if tokens:\n",
    "                sentences.append(tokens)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def load_conllu_gold_with_speaker_role(file_path):\n",
    "    \"\"\"\n",
    "    Load a CONLL-U file and return a list of sentences.\n",
    "    Each sentence is a dict:\n",
    "        {\n",
    "            \"tokens\": [ {token_dict}, ... ],\n",
    "            \"speaker_role\": \"Target_Child\" or other role\n",
    "        }\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    current_tokens = []\n",
    "    speaker_role = None\n",
    "\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line == \"\":\n",
    "                if current_tokens:\n",
    "                    # save the sentence\n",
    "                    sentences.append({\n",
    "                        \"tokens\": current_tokens,\n",
    "                        \"speaker_role\": speaker_role if speaker_role else \"Other_Speakers\"\n",
    "                    })\n",
    "                    current_tokens = []\n",
    "                    speaker_role = None\n",
    "                continue\n",
    "\n",
    "            if line.startswith(\"#\"):\n",
    "                if \"speaker_role\" in line:\n",
    "                    # parse the role from the comment\n",
    "                    speaker_role = line.split(\"=\")[1].strip()\n",
    "                continue\n",
    "\n",
    "            # token line\n",
    "            cols = line.split(\"\\t\")\n",
    "            if len(cols) < 8:\n",
    "                continue  # skip malformed lines\n",
    "\n",
    "            token_id = cols[0]\n",
    "            if \"-\" in token_id or \".\" in token_id:\n",
    "                continue  # skip multiword tokens or empty nodes\n",
    "\n",
    "            head_str = cols[6]\n",
    "            head = int(head_str) if head_str.isdigit() else 0\n",
    "\n",
    "            token = {\n",
    "                \"id\": token_id,\n",
    "                \"form\": cols[1],\n",
    "                \"lemma\": cols[2],\n",
    "                \"upos\": cols[3],\n",
    "                \"xpos\": cols[4],\n",
    "                \"head\": head,\n",
    "                \"deprel\": cols[7],\n",
    "            }\n",
    "            current_tokens.append(token)\n",
    "\n",
    "        # catch last sentence if file does not end with empty line\n",
    "        if current_tokens:\n",
    "            sentences.append({\n",
    "                \"tokens\": current_tokens,\n",
    "                \"speaker_role\": speaker_role if speaker_role else \"Other_Speakers\"\n",
    "            })\n",
    "\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def _normalize_form(s):\n",
    "    \"\"\"Normalize surface forms for loose matching (lowercase, unify quotes).\"\"\"\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = s.replace(\"’\", \"'\").replace(\"‘\", \"'\").replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec99a1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_parsers_overall(test_path, nlp_stanza_childes, log_predictions=False, log_path=\"parser_predictions\"):\n",
    "    results = {}\n",
    "\n",
    "    #nlp_spacy_eng = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    parsers = {\n",
    "        \n",
    "        \"Stanza_off_the_shelf\": stanza.Pipeline(lang='en', processors='tokenize,pos,lemma,depparse', use_gpu=True),\n",
    "        \"Stanza_Custom\": nlp_stanza_childes,\n",
    "\n",
    "    }\n",
    "\n",
    "    gold_sentences = load_conllu_gold(test_path)\n",
    "\n",
    "    log_file_handles = {}\n",
    "    if log_predictions:\n",
    "        base = Path(log_path).stem\n",
    "        out_dir = Path(log_path).parent if Path(log_path).parent.exists() else Path(\".\")\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for parser_name, nlp in parsers.items():\n",
    "        print(nlp.config)\n",
    "        if log_predictions:\n",
    "            safe_name = re.sub(r\"[^0-9A-Za-z_\\-]\", \"_\", parser_name)\n",
    "            log_fp = out_dir / f\"{base}_{safe_name}.conllu\"\n",
    "            lf = open(log_fp, \"w\", encoding=\"utf-8\")\n",
    "            log_file_handles[parser_name] = lf\n",
    "            lf.write(f\"# Parser: {parser_name}\\n\")\n",
    "\n",
    "        correct_upos = 0\n",
    "        correct_xpos = 0\n",
    "        correct_heads = 0\n",
    "        correct_labels = 0\n",
    "        total_tokens = 0\n",
    "\n",
    "        if log_predictions:\n",
    "            lf.write(\"\\n# === Start sentences ===\\n\\n\")\n",
    "\n",
    "        for gold_tokens in tqdm(gold_sentences, desc=f\"Parsing with {parser_name}\"):\n",
    "            gold_text = \" \".join(t[\"form\"] for t in gold_tokens)\n",
    "\n",
    "            if parser_name in [\"Stanza_Custom\", \"Stanza_off_the_shelf\"]:\n",
    "                doc = nlp(gold_text)\n",
    "                pred_tokens = []\n",
    "                for sent in doc.sentences:\n",
    "                    for word in sent.words:\n",
    "                        pred_tokens.append({\n",
    "                            \"form\": word.text,\n",
    "                            \"upos\": word.upos,\n",
    "                            \"xpos\": word.xpos,\n",
    "                            \"head\": int(word.head) if getattr(word, \"head\", None) is not None else 0,\n",
    "                            \"deprel\": word.deprel\n",
    "                        })\n",
    "            else:\n",
    "                doc = nlp(gold_text)\n",
    "                pred_tokens = []\n",
    "                for token in doc:\n",
    "                    head = 0 if token.head.i == token.i else token.head.i + 1\n",
    "                    pred_tokens.append({\n",
    "                        \"form\": token.text,\n",
    "                        \"upos\": token.pos_,\n",
    "                        \"xpos\": token.tag_,\n",
    "                        \"head\": head,\n",
    "                        \"deprel\": token.dep_\n",
    "                    })\n",
    "\n",
    "            if log_predictions:\n",
    "                lf.write(f\"# sentence: {gold_text}\\n\")\n",
    "\n",
    "            pairs = []\n",
    "            if len(gold_tokens) == len(pred_tokens):\n",
    "                pairs = list(zip(gold_tokens, pred_tokens))\n",
    "            else:\n",
    "                pred_idx = 0\n",
    "                for g in gold_tokens:\n",
    "                    g_norm = _normalize_form(g[\"form\"])\n",
    "                    found = None\n",
    "                    for j in range(pred_idx, min(pred_idx + 6, len(pred_tokens))):\n",
    "                        if _normalize_form(pred_tokens[j][\"form\"]) == g_norm:\n",
    "                            found = j\n",
    "                            break\n",
    "                    if found is not None:\n",
    "                        pairs.append((g, pred_tokens[found]))\n",
    "                        pred_idx = found + 1\n",
    "                    else:\n",
    "                        if pred_idx < len(pred_tokens):\n",
    "                            pairs.append((g, pred_tokens[pred_idx]))\n",
    "                            pred_idx += 1\n",
    "                        else:\n",
    "                            pairs.append((g, None))\n",
    "\n",
    "            for g, p in pairs:\n",
    "                total_tokens += 1\n",
    "                gold_upos = g.get(\"upos\", \"\")\n",
    "                gold_xpos = g.get(\"xpos\", \"\")\n",
    "                gold_head = int(g.get(\"head\", 0))\n",
    "                gold_label = g.get(\"deprel\", \"\")\n",
    "\n",
    "                if p is None:\n",
    "                    if log_predictions:\n",
    "                        lf.write(f\"\\nGold: {g}\\nPred: <NO PREDICTION>\\n\")\n",
    "                    continue\n",
    "\n",
    "                pred_upos = p.get(\"upos\", \"\")\n",
    "                pred_xpos = p.get(\"xpos\", \"\")\n",
    "                pred_head = int(p.get(\"head\", 0))\n",
    "                pred_label = p.get(\"deprel\", \"\")\n",
    "\n",
    "                if gold_upos == 'INTJ' and gold_label in ['discourse', 'reparandum']:\n",
    "                    gold_label = 'intj'\n",
    "\n",
    "                if gold_label == 'acl:relcl':\n",
    "                    gold_label = 'relcl'\n",
    "\n",
    "                if pred_label == 'acl:relcl':\n",
    "                    pred_label = 'relcl'\n",
    "                \n",
    "                if gold_label == 'compound:prt':\n",
    "                    gold_label = 'prt'\n",
    "\n",
    "                if pred_label == 'compound:prt':\n",
    "                    pred_label = 'prt' \n",
    "                \n",
    "                if gold_label == 'cc:preconj':\n",
    "                    gold_label = 'preconj'\n",
    "\n",
    "                if pred_label == 'cc:preconj':\n",
    "                    pred_label = 'preconj' \n",
    "                \n",
    "                if gold_label == 'nmod:poss':\n",
    "                    gold_label = 'poss'\n",
    "\n",
    "                if pred_label == 'nmod:poss':\n",
    "                    pred_label = 'poss' \n",
    "                \n",
    "                if gold_label == 'obl:npmod':\n",
    "                    gold_label = 'npadvmod'\n",
    "\n",
    "                if pred_label == 'obl:npmod':\n",
    "                    pred_label = 'npadvmod'\n",
    "\n",
    "                if pred_label in ['dobj', 'pobj'] and gold_label == 'obj':\n",
    "                    pred_label = 'obj'\n",
    "\n",
    "                if pred_label == 'neg':\n",
    "                    pred_label = 'advmod'\n",
    "\n",
    "                if pred_label == 'ROOT':\n",
    "                    pred_label = 'root'\n",
    "                \n",
    "                # UPOS\n",
    "                if pred_upos and pred_upos.lower() == gold_upos.lower():\n",
    "                    correct_upos += 1\n",
    "\n",
    "                # XPOS\n",
    "                if pred_xpos and pred_xpos == gold_xpos:\n",
    "                    correct_xpos += 1\n",
    "\n",
    "                # Heads\n",
    "                if pred_head == gold_head:\n",
    "                    correct_heads += 1\n",
    "                \n",
    "                \n",
    "                if pred_head == gold_head and pred_label == gold_label:\n",
    "                    correct_labels += 1\n",
    "\n",
    "                if log_predictions:                    lf.write(\n",
    "                        f\"\\nGold: token={g['form']}, upos={gold_upos}, xpos={gold_xpos}, head={gold_head}, label={gold_label}\\n\"\n",
    "                        f\"Pred: token={p['form']}, upos={pred_upos}, xpos={pred_xpos}, head={pred_head}, label={pred_label}\\n\"\n",
    "                    )\n",
    "            if log_predictions:\n",
    "                lf.write(\"\\n\" + (\"-\" * 60) + \"\\n\\n\")\n",
    "\n",
    "        if log_predictions:\n",
    "            lf.close()\n",
    "\n",
    "        results[parser_name] = {\n",
    "            \"UPOS Accuracy\": correct_upos / total_tokens if total_tokens > 0 else 0.0,\n",
    "            \"XPOS Accuracy\": correct_xpos / total_tokens if total_tokens > 0 else 0.0,\n",
    "            \"UAS\": correct_heads / total_tokens if total_tokens > 0 else 0.0,\n",
    "            \"LAS\": correct_labels / total_tokens if total_tokens > 0 else 0.0,\n",
    "            \"Total Tokens\": total_tokens\n",
    "        }\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e56eb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_parsers_speaker_roles(test_path, nlp_stanza_childes, log_predictions=False, log_path=\"parser_predictions\"):\n",
    "\n",
    "    #nlp_spacy_eng = spacy.load(\"en_core_web_sm\")\n",
    "    results = {}\n",
    "\n",
    "    parsers = {\n",
    "        \"Stanza_off_the_shelf\": stanza.Pipeline(lang='en', processors='tokenize,pos,lemma,depparse', use_gpu=True),\n",
    "        \"Stanza_Custom\": nlp_stanza_childes,\n",
    "    }\n",
    "\n",
    "    gold_sentences = load_conllu_gold_with_speaker_role(test_path)  # Assume each sentence includes 'speaker_role'\n",
    "\n",
    "    log_file_handles = {}\n",
    "    if log_predictions:\n",
    "        base = Path(log_path).stem\n",
    "        out_dir = Path(log_path).parent if Path(log_path).parent.exists() else Path(\".\")\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for parser_name, nlp in parsers.items():\n",
    "        print(nlp.config)\n",
    "        if log_predictions:\n",
    "            safe_name = re.sub(r\"[^0-9A-Za-z_\\-]\", \"_\", parser_name)\n",
    "            log_fp = out_dir / f\"{base}_{safe_name}.conllu\"\n",
    "            lf = open(log_fp, \"w\", encoding=\"utf-8\")\n",
    "            log_file_handles[parser_name] = lf\n",
    "            lf.write(f\"# Parser: {parser_name}\\n\")\n",
    "\n",
    "        # Two sets of counters\n",
    "        metrics = {\n",
    "            \"Target_Child\": {\"correct_upos\": 0, \"correct_xpos\": 0, \"correct_heads\": 0, \"correct_labels\": 0, \"total_tokens\": 0},\n",
    "            \"Other_Speakers\": {\"correct_upos\": 0, \"correct_xpos\": 0, \"correct_heads\": 0, \"correct_labels\": 0, \"total_tokens\": 0}\n",
    "        }\n",
    "\n",
    "        if log_predictions:\n",
    "            lf.write(\"\\n# === Start sentences ===\\n\\n\")\n",
    "\n",
    "        for gold_sentence in tqdm(gold_sentences, desc=f\"Parsing with {parser_name}\"):\n",
    "            gold_tokens = gold_sentence[\"tokens\"]\n",
    "            speaker_role = gold_sentence.get(\"speaker_role\", \"\")\n",
    "            set_key = \"Target_Child\" if speaker_role == \"Target_Child\" else \"Other_Speakers\"\n",
    "\n",
    "            gold_text = \" \".join(t[\"form\"] for t in gold_tokens)\n",
    "\n",
    "            if parser_name in [\"Stanza_Custom\", \"Stanza_off_the_shelf\"]:\n",
    "                doc = nlp(gold_text)\n",
    "                pred_tokens = []\n",
    "                for sent in doc.sentences:\n",
    "                    for word in sent.words:\n",
    "                        pred_tokens.append({\n",
    "                            \"form\": word.text,\n",
    "                            \"upos\": word.upos,\n",
    "                            \"xpos\": word.xpos,\n",
    "                            \"head\": int(word.head) if getattr(word, \"head\", None) is not None else 0,\n",
    "                            \"deprel\": word.deprel\n",
    "                        })\n",
    "            else:\n",
    "                doc = nlp(gold_text)\n",
    "                pred_tokens = []\n",
    "                for token in doc:\n",
    "                    head = 0 if token.head.i == token.i else token.head.i + 1\n",
    "                    pred_tokens.append({\n",
    "                        \"form\": token.text,\n",
    "                        \"upos\": token.pos_,\n",
    "                        \"xpos\": token.tag_,\n",
    "                        \"head\": head,\n",
    "                        \"deprel\": token.dep_\n",
    "                    })\n",
    "\n",
    "            pairs = []\n",
    "            if len(gold_tokens) == len(pred_tokens):\n",
    "                pairs = list(zip(gold_tokens, pred_tokens))\n",
    "            else:\n",
    "                pred_idx = 0\n",
    "                for g in gold_tokens:\n",
    "                    g_norm = _normalize_form(g[\"form\"])\n",
    "                    found = None\n",
    "                    for j in range(pred_idx, min(pred_idx + 6, len(pred_tokens))):\n",
    "                        if _normalize_form(pred_tokens[j][\"form\"]) == g_norm:\n",
    "                            found = j\n",
    "                            break\n",
    "                    if found is not None:\n",
    "                        pairs.append((g, pred_tokens[found]))\n",
    "                        pred_idx = found + 1\n",
    "                    else:\n",
    "                        if pred_idx < len(pred_tokens):\n",
    "                            pairs.append((g, pred_tokens[pred_idx]))\n",
    "                            pred_idx += 1\n",
    "                        else:\n",
    "                            pairs.append((g, None))\n",
    "\n",
    "            for g, p in pairs:\n",
    "                metrics[set_key][\"total_tokens\"] += 1\n",
    "\n",
    "                gold_upos = g.get(\"upos\", \"\")\n",
    "                gold_xpos = g.get(\"xpos\", \"\")\n",
    "                gold_head = int(g.get(\"head\", 0))\n",
    "                gold_label = g.get(\"deprel\", \"\")\n",
    "\n",
    "                if p is None:\n",
    "                    continue\n",
    "\n",
    "                pred_upos = p.get(\"upos\", \"\")\n",
    "                pred_xpos = p.get(\"xpos\", \"\")\n",
    "                pred_head = int(p.get(\"head\", 0))\n",
    "                pred_label = p.get(\"deprel\", \"\")\n",
    "\n",
    "                # normalize labels as before\n",
    "                if gold_upos == 'INTJ' and gold_label in ['discourse', 'reparandum']:\n",
    "                    gold_label = 'intj'\n",
    "                if gold_label == 'acl:relcl': gold_label = 'relcl'\n",
    "                if pred_label == 'acl:relcl': pred_label = 'relcl'\n",
    "                if gold_label == 'compound:prt': gold_label = 'prt'\n",
    "                if pred_label == 'compound:prt': pred_label = 'prt'\n",
    "                if gold_label == 'cc:preconj': gold_label = 'preconj'\n",
    "                if pred_label == 'cc:preconj': pred_label = 'preconj'\n",
    "                if gold_label == 'nmod:poss': gold_label = 'poss'\n",
    "                if pred_label == 'nmod:poss': pred_label = 'poss'\n",
    "                if gold_label == 'obl:npmod': gold_label = 'npadvmod'\n",
    "                if pred_label == 'obl:npmod': pred_label = 'npadvmod'\n",
    "                if pred_label in ['dobj', 'pobj'] and gold_label == 'obj': pred_label = 'obj'\n",
    "                if pred_label == 'neg': pred_label = 'advmod'\n",
    "                if pred_label == 'ROOT': pred_label = 'root'\n",
    "\n",
    "                # Update counters\n",
    "                if pred_upos and pred_upos.lower() == gold_upos.lower():\n",
    "                    metrics[set_key][\"correct_upos\"] += 1\n",
    "                if pred_xpos and pred_xpos == gold_xpos:\n",
    "                    metrics[set_key][\"correct_xpos\"] += 1\n",
    "                if pred_head == gold_head:\n",
    "                    metrics[set_key][\"correct_heads\"] += 1\n",
    "                if pred_head == gold_head and pred_label == gold_label:\n",
    "                    metrics[set_key][\"correct_labels\"] += 1\n",
    "\n",
    "        # Compute final accuracies for each set\n",
    "        results[parser_name] = {}\n",
    "        for set_key, m in metrics.items():\n",
    "            total = m[\"total_tokens\"] if m[\"total_tokens\"] > 0 else 1\n",
    "            results[parser_name][set_key] = {\n",
    "                \"UPOS Accuracy\": m[\"correct_upos\"] / total,\n",
    "                \"XPOS Accuracy\": m[\"correct_xpos\"] / total,\n",
    "                \"UAS\": m[\"correct_heads\"] / total,\n",
    "                \"LAS\": m[\"correct_labels\"] / total,\n",
    "                \"Total Tokens\": m[\"total_tokens\"]\n",
    "            }\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d063ba07",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"./data/depparse/en_childes.test.in.conllu\"\n",
    "pos_tagger_model = \"./saved_models/pos/en_childes_charlm_tagger.pt\"\n",
    "parser_model = \"./saved_models/depparse/en_childes_charlm_parser.pt\"\n",
    "\n",
    "nlp_childes = stanza.Pipeline(\n",
    "        lang='en',\n",
    "        processors='tokenize,pos,lemma,depparse',\n",
    "        use_gpu=True,\n",
    "        pos_model_path=pos_tagger_model,\n",
    "        depparse_model_path=parser_model\n",
    "    )\n",
    "\n",
    "metrics = evaluate_parsers_speaker_roles(test_path, nlp_childes, log_predictions=True, log_path=\"parser_predictions.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4a2a533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Stanza_off_the_shelf ===\n",
      "Target_Child: {'UPOS Accuracy': 0.9563476788051428, 'XPOS Accuracy': 0.9341750712141043, 'UAS': 0.8762029409500346, 'LAS': 0.8135730233274309, 'Total Tokens': 25978}\n",
      "Other_Speakers: {'UPOS Accuracy': 0.9720577939023614, 'XPOS Accuracy': 0.9311310546750684, 'UAS': 0.8997517662784037, 'LAS': 0.8391254535039144, 'Total Tokens': 31422}\n"
     ]
    }
   ],
   "source": [
    "for parser, scores in metrics.items():\n",
    "    print(f\"\\n=== {parser} ===\")\n",
    "    for metric, value in scores.items():\n",
    "        if isinstance(value, (int, float)):\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"{metric}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
