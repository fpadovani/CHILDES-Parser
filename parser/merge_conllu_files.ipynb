{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c68122b",
   "metadata": {},
   "source": [
    "### Merging [EWT, GUM, GUMReddit, PUD, Pronouns] to obtain the Combined set of UD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d506fd3c",
   "metadata": {},
   "source": [
    "### First we split PUD and Pronouns into train, dev and test (80,10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd902312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "from conllu import parse_incr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75161d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_conllu_file_safe(input_path, output_dir, train_ratio=0.8, dev_ratio=0.1, test_ratio=0.1, seed=42):\n",
    "    input_path = Path(input_path)\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Read sentences properly\n",
    "    with input_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        sentences = list(parse_incr(f))\n",
    "\n",
    "    random.seed(seed)\n",
    "    random.shuffle(sentences)\n",
    "\n",
    "    n = len(sentences)\n",
    "    n_train = int(n * train_ratio)\n",
    "    n_dev = int(n * dev_ratio)\n",
    "    n_test = n - n_train - n_dev\n",
    "\n",
    "    train_sents = sentences[:n_train]\n",
    "    dev_sents = sentences[n_train:n_train+n_dev]\n",
    "    test_sents = sentences[n_train+n_dev:]\n",
    "\n",
    "    def save(sentences_list, path):\n",
    "        with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            for sent in sentences_list:\n",
    "                f.write(sent.serialize())  # <- use serialize() method\n",
    "\n",
    "    save(train_sents, output_dir / f\"{input_path.stem}-train.conllu\")\n",
    "    save(dev_sents, output_dir / f\"{input_path.stem}-dev.conllu\")\n",
    "    save(test_sents, output_dir / f\"{input_path.stem}-test.conllu\")\n",
    "\n",
    "    print(f\"‚úÖ Done! Saved to {output_dir}\")\n",
    "    print(f\"Train: {len(train_sents)} sentences\")\n",
    "    print(f\"Dev:   {len(dev_sents)} sentences\")\n",
    "    print(f\"Test:  {len(test_sents)} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0739a1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_conllu_file_safe_easy(input_path, output_dir, train_ratio=0.8, dev_ratio=0.1, test_ratio=0.1, seed=42):\n",
    "    \"\"\"\n",
    "    Safely splits a .conllu file into train/dev/test sets based on sentence boundaries.\n",
    "    Works even for large or slightly malformed files.\n",
    "    \"\"\"\n",
    "    input_path = Path(input_path)\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"üìñ Reading sentences from {input_path} ...\")\n",
    "\n",
    "    # Read file as text\n",
    "    with input_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read().strip()\n",
    "\n",
    "    # Split on blank lines (standard CoNLL-U sentence separator)\n",
    "    raw_sentences = [s.strip() for s in text.split(\"\\n\\n\") if s.strip()]\n",
    "    print(f\"‚úÖ Found {len(raw_sentences)} sentences.\")\n",
    "\n",
    "    # Filter out malformed ones (not 10 columns in token lines)\n",
    "    sentences = []\n",
    "    bad = 0\n",
    "    for sent in raw_sentences:\n",
    "        lines = sent.splitlines()\n",
    "        token_lines = [l for l in lines if l and not l.startswith(\"#\")]\n",
    "        if all(len(l.split(\"\\t\")) == 10 for l in token_lines):\n",
    "            sentences.append(sent)\n",
    "        else:\n",
    "            bad += 1\n",
    "    if bad > 0:\n",
    "        print(f\"‚ö†Ô∏è Skipped {bad} malformed sentences (not 10 columns).\")\n",
    "\n",
    "    random.seed(seed)\n",
    "    random.shuffle(sentences)\n",
    "\n",
    "    n = len(sentences)\n",
    "    n_train = int(n * train_ratio)\n",
    "    n_dev = int(n * dev_ratio)\n",
    "    n_test = n - n_train - n_dev\n",
    "\n",
    "    train_sents = sentences[:n_train]\n",
    "    dev_sents = sentences[n_train:n_train + n_dev]\n",
    "    test_sents = sentences[n_train + n_dev:]\n",
    "\n",
    "    def save(sent_list, filename):\n",
    "        path = output_dir / filename\n",
    "        with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\\n\".join(sent_list) + \"\\n\")\n",
    "        print(f\"üíæ Saved {len(sent_list)} sentences to {path}\")\n",
    "\n",
    "    base = input_path.stem\n",
    "    save(train_sents, f\"{base}_train.conllu\")\n",
    "    save(dev_sents, f\"{base}_dev.conllu\")\n",
    "    save(test_sents, f\"{base}_test.conllu\")\n",
    "\n",
    "    print(\"\\nüéâ Done splitting!\")\n",
    "    print(f\"Train: {len(train_sents)} | Dev: {len(dev_sents)} | Test: {len(test_sents)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "13c5326b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Done! Saved to /Users/frapadovani/Desktop/stanza/parser/conllu_files_original/splits\n",
      "Train: 228 sentences\n",
      "Dev:   28 sentences\n",
      "Test:  29 sentences\n"
     ]
    }
   ],
   "source": [
    "split_conllu_file_safe(\n",
    "    input_path=\"/Users/frapadovani/Desktop/stanza/parser/conllu_files_original/en_pronouns-ud-test.conllu\",\n",
    "    output_dir=\"/Users/frapadovani/Desktop/stanza/parser/conllu_files_original/splits\",\n",
    "    train_ratio=0.8,\n",
    "    dev_ratio=0.1,\n",
    "    test_ratio=0.1,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "40ada357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Done! Saved to /Users/frapadovani/Desktop/stanza/parser/conllu_files_original/splits\n",
      "Train: 800 sentences\n",
      "Dev:   100 sentences\n",
      "Test:  100 sentences\n"
     ]
    }
   ],
   "source": [
    "split_conllu_file_safe(\n",
    "    input_path=\"/Users/frapadovani/Desktop/stanza/parser/conllu_files_original/en_pud-ud-test.conllu\",\n",
    "    output_dir=\"/Users/frapadovani/Desktop/stanza/parser/conllu_files_original/splits\",\n",
    "    train_ratio=0.8,\n",
    "    dev_ratio=0.1,\n",
    "    test_ratio=0.1,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b797e85a",
   "metadata": {},
   "source": [
    "## Merge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "500c69d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_conllu_files(file_list, output_path):\n",
    "    \"\"\"\n",
    "    Merge multiple CoNLL-U files into a single file.\n",
    "\n",
    "    Args:\n",
    "        file_list (list of str): Paths to CoNLL-U files to merge.\n",
    "        output_path (str): Path to save the merged CoNLL-U file.\n",
    "    \"\"\"\n",
    "    output_path = Path(output_path)\n",
    "    with output_path.open(\"w\", encoding=\"utf-8\") as fout:\n",
    "        for file_path in file_list:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as fin:\n",
    "                content = fin.read().strip()\n",
    "                fout.write(content + \"\\n\\n\")  # ensure blank line between sentences\n",
    "    print(f\"‚úÖ Merged {len(file_list)} files into {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "66f993a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Merged 5 files into /Users/frapadovani/Desktop/stanza/parser/conllu_files_merged/combined_train.conllu\n",
      "‚úÖ Merged 5 files into /Users/frapadovani/Desktop/stanza/parser/conllu_files_merged/combined_dev.conllu\n",
      "‚úÖ Merged 5 files into /Users/frapadovani/Desktop/stanza/parser/conllu_files_merged/combined_test.conllu\n"
     ]
    }
   ],
   "source": [
    "train_files_list = [\n",
    "    \"/Users/frapadovani/Desktop/stanza/parser/conllu_files_original/en_gum-ud-train.conllu\",\n",
    "    \"/Users/frapadovani/Desktop/stanza/parser/conllu_files_original/en_gumreddit-ud-train.conllu\",\n",
    "    \"/Users/frapadovani/Desktop/stanza/parser/conllu_files_original/en_ewt-ud-train.conllu\",\n",
    "    \"/Users/frapadovani/Desktop/stanza/parser/conllu_files_original/splits/en_pud-ud-test-train.conllu\",\n",
    "    \"/Users/frapadovani/Desktop/stanza/parser/conllu_files_original/splits/en_pronouns-ud-test-train.conllu\"\n",
    "]\n",
    "\n",
    "dev_files_list = [\n",
    "    \"/Users/frapadovani/Desktop/stanza/parser/conllu_files_original/en_gum-ud-dev.conllu\",\n",
    "    \"/Users/frapadovani/Desktop/stanza/parser/conllu_files_original/en_gumreddit-ud-dev.conllu\",\n",
    "    \"/Users/frapadovani/Desktop/stanza/parser/conllu_files_original/en_ewt-ud-dev.conllu\",\n",
    "    \"/Users/frapadovani/Desktop/stanza/parser/conllu_files_original/splits/en_pud-ud-test-dev.conllu\",\n",
    "    \"/Users/frapadovani/Desktop/stanza/parser/conllu_files_original/splits/en_pronouns-ud-test-dev.conllu\"\n",
    "]\n",
    "\n",
    "test_files_list = [\n",
    "    \"/Users/frapadovani/Desktop/stanza/parser/conllu_files_original/en_gum-ud-test.conllu\",\n",
    "    \"/Users/frapadovani/Desktop/stanza/parser/conllu_files_original/en_gumreddit-ud-test.conllu\",\n",
    "    \"/Users/frapadovani/Desktop/stanza/parser/conllu_files_original/en_ewt-ud-test.conllu\",\n",
    "    \"/Users/frapadovani/Desktop/stanza/parser/conllu_files_original/splits/en_pud-ud-test-test.conllu\",\n",
    "    \"/Users/frapadovani/Desktop/stanza/parser/conllu_files_original/splits/en_pronouns-ud-test-test.conllu\"\n",
    "]\n",
    "\n",
    "# Merge train/dev/test files\n",
    "merge_conllu_files(train_files_list, \"/Users/frapadovani/Desktop/stanza/parser/conllu_files_merged/combined_train.conllu\")\n",
    "merge_conllu_files(dev_files_list, \"/Users/frapadovani/Desktop/stanza/parser/conllu_files_merged/combined_dev.conllu\")\n",
    "merge_conllu_files(test_files_list, \"/Users/frapadovani/Desktop/stanza/parser/conllu_files_merged/combined_test.conllu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64a7064",
   "metadata": {},
   "source": [
    "## Silver CHILDES UD Annotations (./stanza/UD_CHILDES_silver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aedabc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Merged 11 files into /Users/frapadovani/Desktop/stanza/parser/silver_files_merged/childes_silver.conllu\n"
     ]
    }
   ],
   "source": [
    "silver_data_list = ['/Users/frapadovani/Desktop/stanza/UD_CHILDES_silver/Abe_kuczaj_eud_silver.conllu',\n",
    "                    '/Users/frapadovani/Desktop/stanza/UD_CHILDES_silver/Adam_Brown_eud_silver.conllu',\n",
    "                    '/Users/frapadovani/Desktop/stanza/UD_CHILDES_silver/Brown_Eve_eud_silver.conllu',\n",
    "                    '/Users/frapadovani/Desktop/stanza/UD_CHILDES_silver/Emma_Weist_eud_silver.conllu',\n",
    "                    '/Users/frapadovani/Desktop/stanza/UD_CHILDES_silver/Laura_Braunwald_eud_silver.conllu',\n",
    "                    '/Users/frapadovani/Desktop/stanza/UD_CHILDES_silver/Lily_Providence_eud_silver.conllu',\n",
    "                    '/Users/frapadovani/Desktop/stanza/UD_CHILDES_silver/Naima_Providence_eud_silver.conllu',\n",
    "                    '/Users/frapadovani/Desktop/stanza/UD_CHILDES_silver/Roman_Weist_eud_silver.conllu',\n",
    "                    '/Users/frapadovani/Desktop/stanza/UD_CHILDES_silver/Sarah_Brown_eud_silver.conllu',\n",
    "                    '/Users/frapadovani/Desktop/stanza/UD_CHILDES_silver/Thomas_Thomas_eud_silver.conllu',\n",
    "                    '/Users/frapadovani/Desktop/stanza/UD_CHILDES_silver/Violet_Providence_eud_silver.conllu']\n",
    "\n",
    "merge_conllu_files(silver_data_list, \"/Users/frapadovani/Desktop/stanza/parser/silver_files_merged/childes_silver.conllu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "351fd658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Reading sentences from /Users/frapadovani/Desktop/stanza/parser/silver_files_merged/childes_silver.conllu ...\n",
      "‚úÖ Found 1197471 sentences.\n",
      "üíæ Saved 957976 sentences to /Users/frapadovani/Desktop/stanza/parser/silver_files_merged/childes_silver_train.conllu\n",
      "üíæ Saved 119747 sentences to /Users/frapadovani/Desktop/stanza/parser/silver_files_merged/childes_silver_dev.conllu\n",
      "üíæ Saved 119748 sentences to /Users/frapadovani/Desktop/stanza/parser/silver_files_merged/childes_silver_test.conllu\n",
      "\n",
      "üéâ Done splitting!\n",
      "Train: 957976 | Dev: 119747 | Test: 119748\n"
     ]
    }
   ],
   "source": [
    "split_conllu_file_safe_easy(\n",
    "    input_path=\"/Users/frapadovani/Desktop/stanza/parser/silver_files_merged/childes_silver.conllu\",\n",
    "    output_dir=\"/Users/frapadovani/Desktop/stanza/parser/silver_files_merged\",\n",
    "    train_ratio=0.8,\n",
    "    dev_ratio=0.1,\n",
    "    test_ratio=0.1,\n",
    "    seed=42\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "supar_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
